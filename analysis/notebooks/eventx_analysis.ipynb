{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eventx\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "from itertools import chain, accumulate\n",
    "\n",
    "from eventx.predictors.predictor_utils import load_predictor\n",
    "from eventx.models.model_utils import batched_predict_json, batched_predict_instances\n",
    "from eventx.predictors import snorkel_predictor, smartdata_predictor\n",
    "from eventx.util import scorer\n",
    "from eventx.util import utils\n",
    "from eventx import SD4M_RELATION_TYPES, ROLE_LABELS\n",
    "\n",
    "from allennlp.predictors import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_result_dict(row_name, p, r, f1):\n",
    "    tmp_dict = {\n",
    "        'row_name': row_name,\n",
    "        'P': p,\n",
    "        'R': r,\n",
    "        'F1': f1\n",
    "    }\n",
    "    print(tmp_dict)\n",
    "    return tmp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEVICE = -1  # or -1 if no GPU is available\n",
    "\n",
    "MODEL_DIR = \"../../data/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../../data/daystream_corpus/test_sd4m_with_events.jsonl\"\n",
    "PREDICTOR_NAME = \"snorkel-eventx-predictor\"\n",
    "# PREDICTOR_NAME = \"smartdata-eventx-predictor\"\n",
    "    \n",
    "ALLENNLP_MODEL = \"snorkel_bert_v6-first_trigger_check_gold/model.tar.gz\"\n",
    "# ALLENNLP_MODEL = \"plass_bert_gold/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = load_predictor(MODEL_DIR, PREDICTOR_NAME, CUDA_DEVICE, archive_filename=ALLENNLP_MODEL, weights_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs with triggers: 68 \t Docs without triggers (not supported): 98\n"
     ]
    }
   ],
   "source": [
    "instances = []\n",
    "gold_doc_list = []\n",
    "docs_without_triggers = 0\n",
    "docs_with_triggers = 0\n",
    "with io.open(DATASET_PATH) as test_file:\n",
    "    for line in test_file.readlines():\n",
    "        example = json.loads(line)\n",
    "        if any(e['entity_type'].lower() == 'trigger' for e in example['entities']):\n",
    "            gold_doc_list.append(example)\n",
    "            if 'event_triggers' not in example or 'event_roles' not in example:\n",
    "                # Convert ACE events to Snorkel event triggers & roles\n",
    "                example = utils.convert_events(example)\n",
    "            instances.append(predictor._json_to_instance(example))\n",
    "            docs_with_triggers += 1\n",
    "        else:\n",
    "            # print(f\"Document {example['id']} does not contain triggers and is therefore not supported.\")\n",
    "            docs_without_triggers += 1\n",
    "print(f\"Docs with triggers: {docs_with_triggers} \\t Docs without triggers (not supported): {docs_without_triggers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_instances = batched_predict_instances(predictor, instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_results = []  # save results in here to later export as csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFKI spree REScorer adapted to Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_events_batch = [prediction_instance['events'] for prediction_instance in prediction_instances]\n",
    "gold_events_batch = [doc['events'] for doc in gold_doc_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grusdt, B., Nehring, J., & Thomas, P. (2018). Bootstrapping patterns for the detection of mobility related events.\n",
    "> For a detected event to count as true positive the predicted event type must be equal to the gold standard event type and the predicted event span must at least be subsumed by the gold standard event span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'row_name': 'Event Extraction Acc. (Grusdt et al 2018)', 'P': 0.7368421052631579, 'R': 0.6774193548387096, 'F1': 0.7058823529411764}\n",
      "{'row_name': 'Accident (Grusdt et al 2018)', 'P': 0.6666666666666666, 'R': 0.8888888888888888, 'F1': 0.761904761904762}\n",
      "{'row_name': 'CanceledRoute (Grusdt et al 2018)', 'P': 0.5, 'R': 0.36363636363636365, 'F1': 0.4210526315789474}\n",
      "{'row_name': 'CanceledStop (Grusdt et al 2018)', 'P': 0.6666666666666666, 'R': 0.6666666666666666, 'F1': 0.6666666666666666}\n",
      "{'row_name': 'Delay (Grusdt et al 2018)', 'P': 1.0, 'R': 0.3333333333333333, 'F1': 0.5}\n",
      "{'row_name': 'Obstruction (Grusdt et al 2018)', 'P': 0.7, 'R': 0.5833333333333334, 'F1': 0.6363636363636365}\n",
      "{'row_name': 'RailReplacementService (Grusdt et al 2018)', 'P': 0.8, 'R': 0.8, 'F1': 0.8000000000000002}\n",
      "{'row_name': 'TrafficJam (Grusdt et al 2018)', 'P': 0.8823529411764706, 'R': 0.9375, 'F1': 0.9090909090909091}\n"
     ]
    }
   ],
   "source": [
    "results = scorer.score_events_batch(pred_events_batch, gold_events_batch, allow_subsumption=True, keep_event_matches=True, ignore_span=False, ignore_args=True, ignore_optional_args=True)\n",
    "acc_results = results['accumulated']\n",
    "tmp_dict = make_result_dict(\"Event Extraction Acc. (Grusdt et al 2018)\", acc_results.precision(), acc_results.recall(), acc_results.f1())\n",
    "table_results.append(tmp_dict)\n",
    "for event_class in SD4M_RELATION_TYPES[:-1]:\n",
    "    if event_class in results:\n",
    "        class_results = results[event_class]\n",
    "        tmp_dict = make_result_dict(f\"{event_class} (Grusdt et al 2018)\", class_results.precision(), class_results.recall(), class_results.f1())\n",
    "        table_results.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schiersch, M., Mironova, V., Schmitt, M., Thomas, P., Gabryszak, A., & Hennig, L. (2018). A german corpus for fine-grained named entity recognition and relation extraction of traffic and industry events. arXiv preprint arXiv:2004.03283.\n",
    "> \\[W\\]e chose a soft matching strategy that counts a predicted relation mention as correct if all predicted arguments also occur in the corresponding gold relation mention, and if all required arguments have been correctly predicted, based on their role, underlying entity, and character offsets / extent. Optional arguments from the gold relation mention that are not contained in the predicted relation mention do not count as errors. In other words, we count a predicted relation mention as correct if it contains all required arguments and is subsumed by or equal to the gold relation mention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'row_name': 'Event Extraction Acc. (Schiersch et al 2018)', 'P': 0.7017543859649122, 'R': 0.6451612903225806, 'F1': 0.6722689075630253}\n",
      "{'row_name': 'Accident (Schiersch et al 2018)', 'P': 0.6666666666666666, 'R': 0.8888888888888888, 'F1': 0.761904761904762}\n",
      "{'row_name': 'CanceledRoute (Schiersch et al 2018)', 'P': 0.375, 'R': 0.2727272727272727, 'F1': 0.3157894736842105}\n",
      "{'row_name': 'CanceledStop (Schiersch et al 2018)', 'P': 0.6666666666666666, 'R': 0.6666666666666666, 'F1': 0.6666666666666666}\n",
      "{'row_name': 'Delay (Schiersch et al 2018)', 'P': 1.0, 'R': 0.3333333333333333, 'F1': 0.5}\n",
      "{'row_name': 'Obstruction (Schiersch et al 2018)', 'P': 0.6, 'R': 0.5, 'F1': 0.5454545454545454}\n",
      "{'row_name': 'RailReplacementService (Schiersch et al 2018)', 'P': 0.8, 'R': 0.8, 'F1': 0.8000000000000002}\n",
      "{'row_name': 'TrafficJam (Schiersch et al 2018)', 'P': 0.8823529411764706, 'R': 0.9375, 'F1': 0.9090909090909091}\n"
     ]
    }
   ],
   "source": [
    "results = scorer.score_events_batch(pred_events_batch, gold_events_batch, allow_subsumption=True, keep_event_matches=True, ignore_span=False, ignore_args=False, ignore_optional_args=True)\n",
    "acc_results = results['accumulated']\n",
    "tmp_dict = make_result_dict(\"Event Extraction Acc. (Schiersch et al 2018)\", acc_results.precision(), acc_results.recall(), acc_results.f1())\n",
    "table_results.append(tmp_dict)\n",
    "for event_class in SD4M_RELATION_TYPES[:-1]:\n",
    "    if event_class in results:\n",
    "        class_results = results[event_class]\n",
    "        tmp_dict = make_result_dict(f\"{event_class} (Schiersch et al 2018)\", class_results.precision(), class_results.recall(), class_results.f1())\n",
    "        table_results.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event extraction evaluation using correctness criteria defined by Ji, Heng and Grishman, Ralph 2008\n",
    "\n",
    "Ji, Heng and Grishman, Ralph (2008). Refining event extraction through cross-document inference.\n",
    "> A trigger is correctly labeled if its event type and offsets match a reference trigger.\n",
    "\n",
    "> An argument is correctly identified if its event type and offsets match any of the reference argument mentions.\n",
    "\n",
    "> An argument is correctly identified and classified if its event type, offsets, and role match any of the reference argument mentions.\n",
    "\n",
    "Caution:\n",
    "Using the following methods to retrieve the triggers and arguments from the gold data might result in duplicate gold triggers & arguments.\n",
    "This is due to different events possibly sharing the same trigger.\n",
    "The model is not able to distinguish such events and instead fuses them all together, which should result in lower recall.\n",
    "If we remove duplicates from the gold triggers and gold arguments, recall and consequently f1 should be higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_DUPLICATES = True  # change to False if you want to keep duplicate triggers/ arguments from the gold data caused by events sharing the same trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_triggers = scorer.get_triggers(gold_doc_list)\n",
    "gold_arguments = scorer.get_arguments(gold_doc_list)\n",
    "pred_triggers = scorer.get_triggers(prediction_instances)\n",
    "pred_arguments = scorer.get_arguments(prediction_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REMOVE_DUPLICATES:\n",
    "    gold_triggers = list(set(gold_triggers))\n",
    "    gold_arguments = list(set(gold_arguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'row_name': 'Trigger Identification', 'P': 0.8947368421052632, 'R': 0.8947368421052632, 'F1': 0.8947368421052632}\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = scorer.get_trigger_identification_metrics(gold_triggers, pred_triggers)\n",
    "tmp_dict = make_result_dict('Trigger Identification', precision, recall, f1)\n",
    "table_results.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'row_name': 'Trigger Classification', 'P': 0.8947368421052632, 'R': 0.8947368421052632, 'F1': 0.8947368421052632}\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = scorer.get_trigger_classification_metrics(gold_triggers, pred_triggers, accumulated=True)\n",
    "tmp_dict = make_result_dict('Trigger Classification', precision, recall, f1)\n",
    "table_results.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'row_name': 'Accident', 'P': 0.75, 'R': 1.0, 'F1': 0.8571428571428571}\n",
      "{'row_name': 'CanceledRoute', 'P': 1.0, 'R': 0.8, 'F1': 0.888888888888889}\n",
      "{'row_name': 'CanceledStop', 'P': 1.0, 'R': 1.0, 'F1': 1.0}\n",
      "{'row_name': 'Delay', 'P': 1.0, 'R': 1.0, 'F1': 1.0}\n",
      "{'row_name': 'Obstruction', 'P': 1.0, 'R': 0.8333333333333334, 'F1': 0.9090909090909091}\n",
      "{'row_name': 'RailReplacementService', 'P': 0.8, 'R': 0.8, 'F1': 0.8000000000000002}\n",
      "{'row_name': 'TrafficJam', 'P': 0.8823529411764706, 'R': 0.9375, 'F1': 0.9090909090909091}\n"
     ]
    }
   ],
   "source": [
    "class_results = scorer.get_trigger_classification_metrics(gold_triggers, pred_triggers, accumulated=False)\n",
    "for trigger_class in SD4M_RELATION_TYPES[:-1]:\n",
    "    if trigger_class in class_results:\n",
    "        precision, recall, f1 = class_results[trigger_class]\n",
    "    else:\n",
    "        precision, recall, f1 = 0.0, 0.0, 0.0\n",
    "    tmp_dict = make_result_dict(trigger_class, precision, recall, f1)\n",
    "    table_results.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'row_name': 'Argument Identification', 'P': 0.7695473251028807, 'R': 0.7663934426229508, 'F1': 0.7679671457905545}\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = scorer.get_argument_identification_metrics(gold_arguments, pred_arguments)\n",
    "tmp_dict = make_result_dict('Argument Identification', precision, recall, f1)\n",
    "table_results.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'row_name': 'Argument Classification', 'P': 0.7407407407407407, 'R': 0.7377049180327869, 'F1': 0.7392197125256674}\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = scorer.get_argument_classification_metrics(gold_arguments, pred_arguments, accumulated=True)\n",
    "tmp_dict = make_result_dict('Argument Classification', precision, recall, f1)\n",
    "table_results.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'row_name': 'location', 'P': 0.7671232876712328, 'R': 0.6666666666666666, 'F1': 0.713375796178344}\n",
      "{'row_name': 'delay', 'P': 0.8, 'R': 0.5, 'F1': 0.6153846153846154}\n",
      "{'row_name': 'direction', 'P': 0.7058823529411765, 'R': 0.7741935483870968, 'F1': 0.7384615384615385}\n",
      "{'row_name': 'start_loc', 'P': 0.7391304347826086, 'R': 0.8717948717948718, 'F1': 0.7999999999999999}\n",
      "{'row_name': 'end_loc', 'P': 0.66, 'R': 0.8461538461538461, 'F1': 0.7415730337078651}\n",
      "{'row_name': 'start_date', 'P': 1.0, 'R': 0.2222222222222222, 'F1': 0.3636363636363636}\n",
      "{'row_name': 'end_date', 'P': 0.5, 'R': 0.3333333333333333, 'F1': 0.4}\n",
      "{'row_name': 'cause', 'P': 0.7692307692307693, 'R': 0.6666666666666666, 'F1': 0.7142857142857142}\n",
      "{'row_name': 'jam_length', 'P': 0.9285714285714286, 'R': 1.0, 'F1': 0.962962962962963}\n",
      "{'row_name': 'route', 'P': 0.75, 'R': 1.0, 'F1': 0.8571428571428571}\n"
     ]
    }
   ],
   "source": [
    "class_results = scorer.get_argument_classification_metrics(gold_arguments, pred_arguments, accumulated=False)\n",
    "for role_class in ROLE_LABELS[:-1]:\n",
    "    if role_class in class_results:\n",
    "        precision, recall, f1 = class_results[role_class]\n",
    "    else:\n",
    "        precision, recall, f1 = 0.0, 0.0, 0.0\n",
    "    tmp_dict = make_result_dict(role_class, precision, recall, f1)\n",
    "    table_results.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_results_df = pd.DataFrame(table_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_results_df.to_csv(MODEL_DIR + '/table_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (eventx)",
   "language": "python",
   "name": "eventx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
