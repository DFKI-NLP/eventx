{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eventx\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "from itertools import chain, accumulate\n",
    "\n",
    "from eventx.predictors.predictor_utils import load_predictor\n",
    "from eventx.models.model_utils import batched_predict_json, batched_predict_instances\n",
    "from eventx.predictors import snorkel_predictor, smartdata_predictor\n",
    "from eventx.util import scorer\n",
    "from eventx.util.utils import snorkel_to_ace_format\n",
    "from eventx import SD4M_RELATION_TYPES, ROLE_LABELS\n",
    "\n",
    "from allennlp.predictors import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_triggers(doc):\n",
    "    return any(entity['entity_type'] == 'trigger' for entity in doc['entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEVICE = -1  # or -1 if no GPU is available\n",
    "\n",
    "MODEL_DIR = \"../../data/runs\"\n",
    "\n",
    "SNORKEL = True  # set to False to use smartdata-eventx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SNORKEL:\n",
    "    DATASET_PATH = \"../../data/snorkel_new/test_with_events_and_defaults.jsonl\"\n",
    "    PREDICTOR_NAME = \"snorkel-eventx-predictor\"\n",
    "    # ALLENNLP_MODEL = \"snorkel_bert_v6-first_trigger_check_gold/model.tar.gz\"\n",
    "    # ALLENNLP_MODEL = \"snorkel_bert_v6-first_trigger_check_converted_abstains/model.tar.gz\"\n",
    "    ALLENNLP_MODEL = \"snorkel_bert_v6-first-trigger_check_snorkeled_gold_conv_merge_with_abstains/model.tar.gz\"\n",
    "else:\n",
    "    DATASET_PATH = \"../../data/snorkel_new/test_sd4m_with_events.jsonl\"\n",
    "    PREDICTOR_NAME = \"smartdata-eventx-predictor\"\n",
    "    ALLENNLP_MODEL = \"plass_bert_gold/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = load_predictor(MODEL_DIR, PREDICTOR_NAME, CUDA_DEVICE, archive_filename=ALLENNLP_MODEL, weights_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = []\n",
    "docs_without_triggers = 0\n",
    "docs_with_triggers = 0\n",
    "with io.open(DATASET_PATH) as test_file:\n",
    "    for line in test_file.readlines():\n",
    "        example = json.loads(line)\n",
    "        if any(e['entity_type'].lower() == 'trigger' for e in example['entities']):\n",
    "            instances.append(predictor._json_to_instance(example))\n",
    "            docs_with_triggers += 1\n",
    "        else:\n",
    "            # print(f\"Document {example['id']} does not contain triggers and is therefore not supported.\")\n",
    "            docs_without_triggers += 1\n",
    "print(f\"Docs with triggers: {docs_with_triggers} \\t Docs without triggers (not supported): {docs_without_triggers}\")\n",
    "prediction_instances = batched_predict_instances(predictor, instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_as_json_list = []\n",
    "with io.open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        example = json.loads(line)\n",
    "        doc_as_json_list.append(example)\n",
    "filtered_doc_list = [doc for doc in doc_as_json_list if has_triggers(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SNORKEL:\n",
    "    filtered_doc_list = snorkel_to_ace_format(filtered_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_results = []  # save results in here to later export as csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFKI spree REScorer adapted to Python\n",
    "Grusdt, B., Nehring, J., & Thomas, P. (2018). Bootstrapping patterns for the detection of mobility related events.\n",
    "> For a detected event to count as true positive the predicted event type must be equal to the gold standard event type and the predicted event span must at least be subsumed by the gold standard event span.\n",
    "\n",
    "Schiersch, M., Mironova, V., Schmitt, M., Thomas, P., Gabryszak, A., & Hennig, L. (2018). A german corpus for fine-grained named entity recognition and relation extraction of traffic and industry events. arXiv preprint arXiv:2004.03283.\n",
    "> \\[W\\]e chose a soft matching strategy that counts a predicted relation mention as correct if all predicted arguments also occur in the corresponding gold relation mention, and if all required arguments have been correctly predicted, based on their role, underlying entity, and character offsets / extent. Optional arguments from the gold relation mention that are not contained in the predicted relation mention do not count as errors. In other words, we count a predicted relation mention as correct if it contains all required arguments and is subsumed by or equal to the gold relation mention.\n",
    "\n",
    "The scorer does the following:\n",
    "- It goes through every gold event and looks for a matching/ subsumed predicted event using an EventComparator (see below).\n",
    "- It increments the true positive count if such a predicted event is found and increments the false negative count otherwise.\n",
    "- It treats all remaining predicted events, which were not matched with any of the gold events as false positives.\n",
    "\n",
    "The EventComparator compares two events using the following criteria:\n",
    "- Do the event types match?\n",
    "- Do the spans match or is the predicted event subsumed by the gold event?\n",
    "- Optionally: Do all the predicted arguments match any of the gold arguments? (I.e. do the argument spans and argument roles match?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_events_batch = [prediction_instance['events'] for prediction_instance in prediction_instances]\n",
    "gold_events_batch = [doc['events'] for doc in filtered_doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scorer.score_events_batch(pred_events_batch, gold_events_batch, allow_subsumption=True, keep_event_matches=True, ignore_span=False, ignore_args=True, ignore_optional_args=True)\n",
    "acc_results = results['accumulated']\n",
    "tmp_dict = {\n",
    "    'row_name': \"Event Extraction Acc. (Grusdt et al 2018)\",\n",
    "    'P': acc_results.precision(),\n",
    "    'R': acc_results.recall(),\n",
    "    'F1': acc_results.f1()\n",
    "}\n",
    "print(tmp_dict)\n",
    "table_results.append(tmp_dict)\n",
    "for event_class in SD4M_RELATION_TYPES[:-1]:\n",
    "    if event_class in results:\n",
    "        class_results = results[event_class]\n",
    "        tmp_dict = {\n",
    "            'row_name': f\"{event_class} (Grusdt et al 2018)\",\n",
    "            'P': class_results.precision(),\n",
    "            'R': class_results.recall(),\n",
    "            'F1': class_results.f1()\n",
    "        }\n",
    "        print(tmp_dict)\n",
    "        table_results.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schiersch et al 2018\n",
    "\n",
    "results = scorer.score_events_batch(pred_events_batch, gold_events_batch, allow_subsumption=True, keep_event_matches=True, ignore_span=False, ignore_args=False, ignore_optional_args=True)\n",
    "acc_results = results['accumulated']\n",
    "tmp_dict = {\n",
    "    'row_name': \"Event Extraction Acc. (Schiersch et al 2018)\",\n",
    "    'P': acc_results.precision(),\n",
    "    'R': acc_results.recall(),\n",
    "    'F1': acc_results.f1()\n",
    "}\n",
    "print(tmp_dict)\n",
    "table_results.append(tmp_dict)\n",
    "for event_class in SD4M_RELATION_TYPES[:-1]:\n",
    "    if event_class in results:\n",
    "        class_results = results[event_class]\n",
    "        tmp_dict = {\n",
    "            'row_name': f\"{event_class} (Schiersch et al 2018)\",\n",
    "            'P': class_results.precision(),\n",
    "            'R': class_results.recall(),\n",
    "            'F1': class_results.f1()\n",
    "        }\n",
    "        print(tmp_dict)\n",
    "        table_results.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event extraction evaluation using correctness criteria defined by Ji, Heng and Grishman, Ralph 2008\n",
    "\n",
    "Ji, Heng and Grishman, Ralph (2008). Refining event extraction through cross-document inference.\n",
    "> A trigger is correctly labeled if its event type and offsets match a reference trigger.\n",
    "\n",
    "> An argument is correctly identified if its event type and offsets match any of the reference argument mentions.\n",
    "\n",
    "> An argument is correctly identified and classified if its event type, offsets, and role match any of the reference argument mentions.\n",
    "\n",
    "Caution:\n",
    "Using the following methods to retrieve the triggers and arguments from the gold data might result in duplicate gold triggers & arguments.\n",
    "This is due to different events possibly sharing the same trigger.\n",
    "The model is not able to distinguish such events and instead fuses them all together, which should result in lower recall.\n",
    "If we remove duplicates from the gold triggers and gold arguments, recall and consequently f1 should be higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_DUPLICATES = True  # change to False if you want to keep duplicate triggers/ arguments from the gold data caused by events sharing the same trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_triggers = scorer.get_triggers(filtered_doc_list)\n",
    "gold_arguments = scorer.get_arguments(filtered_doc_list)\n",
    "pred_triggers = scorer.get_triggers(prediction_instances)\n",
    "pred_arguments = scorer.get_arguments(prediction_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REMOVE_DUPLICATES:\n",
    "    gold_triggers = list(set(gold_triggers))\n",
    "    gold_arguments = list(set(gold_arguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1 = scorer.get_trigger_identification_metrics(gold_triggers, pred_triggers)\n",
    "tmp_dict = {\n",
    "    'row_name': 'Trigger Identification',\n",
    "    'P': precision,\n",
    "    'R': recall,\n",
    "    'F1': f1\n",
    "}\n",
    "print(tmp_dict)\n",
    "table_results.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1 = scorer.get_trigger_classification_metrics(gold_triggers, pred_triggers, accumulated=True)\n",
    "tmp_dict = {\n",
    "    'row_name': 'Trigger Classification',\n",
    "    'P': precision,\n",
    "    'R': recall,\n",
    "    'F1': f1\n",
    "}\n",
    "print(tmp_dict)\n",
    "table_results.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_results = scorer.get_trigger_classification_metrics(gold_triggers, pred_triggers, accumulated=False)\n",
    "for trigger_class in SD4M_RELATION_TYPES[:-1]:\n",
    "    if trigger_class in class_results:\n",
    "        precision, recall, f1 = class_results[trigger_class]\n",
    "    else:\n",
    "        precision, recall, f1 = 0.0, 0.0, 0.0\n",
    "    tmp_dict = {\n",
    "        'row_name': trigger_class,\n",
    "        'P': precision,\n",
    "        'R': recall,\n",
    "        'F1': f1\n",
    "    }\n",
    "    print(tmp_dict)\n",
    "    table_results.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1 = scorer.get_argument_identification_metrics(gold_arguments, pred_arguments)\n",
    "tmp_dict = {\n",
    "    'row_name': 'Argument Identification',\n",
    "    'P': precision,\n",
    "    'R': recall,\n",
    "    'F1': f1\n",
    "}\n",
    "print(tmp_dict)\n",
    "table_results.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1 = scorer.get_argument_classification_metrics(gold_arguments, pred_arguments, accumulated=True)\n",
    "tmp_dict = {\n",
    "    'row_name': 'Argument Classification',\n",
    "    'P': precision,\n",
    "    'R': recall,\n",
    "    'F1': f1\n",
    "}\n",
    "print(tmp_dict)\n",
    "table_results.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_results = scorer.get_argument_classification_metrics(gold_arguments, pred_arguments, accumulated=False)\n",
    "for role_class in ROLE_LABELS[:-1]:\n",
    "    if role_class in class_results:\n",
    "        precision, recall, f1 = class_results[role_class]\n",
    "    else:\n",
    "        precision, recall, f1 = 0.0, 0.0, 0.0\n",
    "    tmp_dict = {\n",
    "        'row_name': role_class,\n",
    "        'P': precision,\n",
    "        'R': recall,\n",
    "        'F1': f1\n",
    "    }\n",
    "    print(tmp_dict)\n",
    "    table_results.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_results_df = pd.DataFrame(table_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_results_df.to_csv('/Users/phuc/Desktop/table_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (eventx)",
   "language": "python",
   "name": "eventx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
